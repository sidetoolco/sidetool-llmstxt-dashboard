import { NextResponse } from 'next/server'
import { createClient } from '@supabase/supabase-js'

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL!
const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY!

export async function POST(request: Request) {
  try {
    const { job_id } = await request.json()
    
    if (!job_id) {
      return NextResponse.json({ error: 'Job ID required' }, { status: 400 })
    }
    
    const supabase = createClient(supabaseUrl, supabaseServiceKey)
    
    // Get job info
    const { data: job } = await supabase
      .from('crawl_jobs')
      .select('*')
      .eq('id', job_id)
      .single()
    
    if (!job) {
      return NextResponse.json({ error: 'Job not found' }, { status: 404 })
    }
    
    // Check if files already exist
    const { data: existingFiles } = await supabase
      .from('generated_files')
      .select('id')
      .eq('job_id', job_id)
    
    if (existingFiles && existingFiles.length > 0) {
      return NextResponse.json({ 
        message: 'Files already exist for this job',
        file_count: existingFiles.length 
      })
    }
    
    // Get all completed URLs
    const { data: urls } = await supabase
      .from('crawled_urls')
      .select('*')
      .eq('job_id', job_id)
      .eq('status', 'completed')
      .order('url')
    
    if (!urls || urls.length === 0) {
      return NextResponse.json({ 
        error: 'No completed URLs to generate files from',
        message: 'Please wait for URLs to be processed first'
      }, { status: 400 })
    }
    
    const filesToInsert = []
    
    // Generate individual llms.txt file for each page
    for (const url of urls) {
      // Extract page name from URL for filename
      const urlPath = new URL(url.url).pathname
      const pageName = urlPath.split('/').filter(Boolean).pop() || 'index'
      
      // Generate individual llms.txt content
      let pageContent = `# ${url.title}\n\n`
      pageContent += `> ${url.description}\n\n`
      pageContent += `## Overview\n\n`
      pageContent += `This content is from ${url.url}\n\n`
      pageContent += `## Content\n\n`
      
      // Add the actual content with proper formatting
      if (url.content) {
        // Clean and format the content
        const cleanContent = url.content
          .replace(/\n{3,}/g, '\n\n') // Remove excessive line breaks
          .trim()
        
        pageContent += cleanContent
      } else {
        pageContent += 'No content available for this page.'
      }
      
      pageContent += `\n\n---\n\n`
      pageContent += `*Generated by SideGSO on ${new Date().toISOString()}*\n`
      pageContent += `*Source: ${url.url}*`
      
      // Add individual file to insert list
      filesToInsert.push({
        job_id: job_id,
        file_type: 'llms.txt',
        file_path: `${job.domain}/${pageName}-llms.txt`,
        file_size: new TextEncoder().encode(pageContent).length,
        content: pageContent
      })
    }
    
    // Also generate consolidated index file
    let indexContent = `# ${job.domain} - SideGSO Files\n\n`
    indexContent += `Generated: ${new Date().toISOString()}\n`
    indexContent += `Total Pages: ${urls.length}\n\n`
    indexContent += `## Available LLMs.txt Files\n\n`
    
    for (const url of urls) {
      const urlPath = new URL(url.url).pathname
      const pageName = urlPath.split('/').filter(Boolean).pop() || 'index'
      indexContent += `- **${pageName}-llms.txt**: [${url.title}](${url.url})\n`
      indexContent += `  - ${url.description}\n\n`
    }
    
    indexContent += `## How to Use\n\n`
    indexContent += `1. Download the individual llms.txt files for the pages you want\n`
    indexContent += `2. Place them in your website's root directory or appropriate path\n`
    indexContent += `3. The files are now accessible to LLMs for better context about your content\n\n`
    indexContent += `## About SideGSO\n\n`
    indexContent += `SideGSO helps you generate LLM-optimized content files for your website, improving AI understanding of your content.`
    
    filesToInsert.push({
      job_id: job_id,
      file_type: 'llms.txt',
      file_path: `${job.domain}/index-llms.txt`,
      file_size: new TextEncoder().encode(indexContent).length,
      content: indexContent
    })
    
    // Store all files
    const { data: insertedFiles, error: insertError } = await supabase
      .from('generated_files')
      .insert(filesToInsert)
      .select()
    
    if (insertError) {
      console.error('Error inserting files:', insertError)
      return NextResponse.json({ 
        error: `Failed to save files: ${insertError.message}`,
        details: insertError
      }, { status: 500 })
    }
    
    // Calculate total content size
    const totalSize = filesToInsert.reduce((sum, file) => sum + file.file_size, 0)
    
    // Update job status
    const { error: updateError } = await supabase
      .from('crawl_jobs')
      .update({
        status: 'completed',
        completed_at: new Date().toISOString(),
        total_content_size: totalSize
      })
      .eq('id', job_id)
    
    if (updateError) {
      console.error('Error updating job status:', updateError)
    }
    
    return NextResponse.json({
      message: `Successfully generated ${insertedFiles?.length || 0} files`,
      files_created: insertedFiles?.length || 0,
      total_size: totalSize,
      job_status: 'completed'
    })
    
  } catch (error: any) {
    console.error('Generate files error:', error)
    return NextResponse.json({ error: error.message }, { status: 500 })
  }
}