import { NextResponse } from 'next/server'
import { createClient } from '@supabase/supabase-js'

const supabaseUrl = process.env.NEXT_PUBLIC_SUPABASE_URL!
const supabaseServiceKey = process.env.SUPABASE_SERVICE_ROLE_KEY!

export async function POST(request: Request) {
  try {
    const { job_id } = await request.json()
    
    if (!job_id) {
      return NextResponse.json({ error: 'Job ID required' }, { status: 400 })
    }
    
    const supabase = createClient(supabaseUrl, supabaseServiceKey)
    
    // Get job details
    const { data: job } = await supabase
      .from('crawl_jobs')
      .select('*')
      .eq('id', job_id)
      .single()
    
    if (!job) {
      return NextResponse.json({ error: 'Job not found' }, { status: 404 })
    }
    
    // Force delete ALL existing files for this job
    // Use service role to bypass any RLS issues
    const { error: deleteError } = await supabase
      .from('generated_files')
      .delete()
      .eq('job_id', job_id)
    
    if (deleteError) {
      console.log('Delete error (continuing anyway):', deleteError.message)
    }
    
    // Double-check deletion worked
    const { data: remainingFiles } = await supabase
      .from('generated_files')
      .select('id')
      .eq('job_id', job_id)
    
    if (remainingFiles && remainingFiles.length > 0) {
      // Force delete by ID
      for (const file of remainingFiles) {
        await supabase
          .from('generated_files')
          .delete()
          .eq('id', file.id)
      }
    }
    
    // Get all completed URLs
    const { data: urls } = await supabase
      .from('crawled_urls')
      .select('*')
      .eq('job_id', job_id)
      .eq('status', 'completed')
      .order('url')
    
    if (!urls || urls.length === 0) {
      return NextResponse.json({ 
        error: 'No completed URLs to generate files from'
      }, { status: 400 })
    }
    
    const filesToInsert = []
    const usedPaths = new Set<string>()
    
    // Generate individual files with guaranteed unique paths
    for (let i = 0; i < urls.length; i++) {
      const url = urls[i]
      const urlObj = new URL(url.url)
      const pathParts = urlObj.pathname.split('/').filter(Boolean)
      
      // Create a unique filename based on the URL path
      let baseName = pathParts.length > 0 ? pathParts.join('-') : 'home'
      baseName = baseName.replace(/[^a-z0-9-]/gi, '-').toLowerCase()
      
      // Ensure uniqueness
      let fileName = `${job.domain}/${baseName}-llms.txt`
      let counter = 1
      while (usedPaths.has(fileName)) {
        fileName = `${job.domain}/${baseName}-${counter}-llms.txt`
        counter++
      }
      usedPaths.add(fileName)
      
      // Generate content
      let content = `# ${url.title || 'Untitled Page'}\n\n`
      content += `> ${url.description || 'No description available'}\n\n`
      content += `Source: ${url.url}\n\n`
      content += `## Content\n\n`
      
      if (url.content) {
        content += url.content.substring(0, 10000) // Limit content size
      } else {
        content += 'No content available.'
      }
      
      content += `\n\n---\n`
      content += `Generated by SideGSO â€¢ ${new Date().toISOString()}`
      
      filesToInsert.push({
        job_id: job_id,
        file_type: 'llms.txt',
        file_path: fileName,
        file_size: new TextEncoder().encode(content).length,
        content: content
      })
    }
    
    // Add index file with unique name
    const indexPath = `${job.domain}/index-main-llms.txt`
    let indexContent = `# ${job.domain} - SideGSO Generated Files\n\n`
    indexContent += `Generated: ${new Date().toISOString()}\n`
    indexContent += `Total Pages: ${urls.length}\n\n`
    indexContent += `## Files Generated\n\n`
    
    for (const file of filesToInsert) {
      const fileName = file.file_path.split('/').pop()
      indexContent += `- ${fileName}\n`
    }
    
    filesToInsert.push({
      job_id: job_id,
      file_type: 'llms.txt',
      file_path: indexPath,
      file_size: new TextEncoder().encode(indexContent).length,
      content: indexContent
    })
    
    // Insert all files
    const { data: inserted, error: insertError } = await supabase
      .from('generated_files')
      .insert(filesToInsert)
      .select('id, file_path')
    
    if (insertError) {
      return NextResponse.json({ 
        error: `Insert failed: ${insertError.message}`,
        details: insertError,
        attempted: filesToInsert.length
      }, { status: 500 })
    }
    
    // Update job status
    await supabase
      .from('crawl_jobs')
      .update({
        status: 'completed',
        completed_at: new Date().toISOString()
      })
      .eq('id', job_id)
    
    return NextResponse.json({
      success: true,
      message: `Generated ${inserted?.length || 0} files`,
      files_created: inserted?.length || 0,
      file_paths: inserted?.map(f => f.file_path) || []
    })
    
  } catch (error: any) {
    console.error('Generate error:', error)
    return NextResponse.json({ 
      error: error.message,
      stack: error.stack 
    }, { status: 500 })
  }
}