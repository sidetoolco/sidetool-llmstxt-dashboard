export async function generateSimpleFiles(jobId: string, supabase: any) {
  // Get job info
  const { data: job } = await supabase
    .from('crawl_jobs')
    .select('*')
    .eq('id', jobId)
    .single()
  
  // Get all completed URLs
  const { data: urls } = await supabase
    .from('crawled_urls')
    .select('*')
    .eq('job_id', jobId)
    .eq('status', 'completed')
    .order('url')
  
  if (!urls || urls.length === 0) {
    console.log('No completed URLs to generate files from')
    await supabase
      .from('crawl_jobs')
      .update({
        status: 'completed',
        completed_at: new Date().toISOString()
      })
      .eq('id', jobId)
    return
  }
  
  const filesToInsert = []
  
  // Generate individual llms.txt file for each page
  for (const url of urls) {
    // Extract page name from URL for filename
    const urlPath = new URL(url.url).pathname
    const pageName = urlPath.split('/').filter(Boolean).pop() || 'index'
    
    // Generate individual llms.txt content
    let pageContent = `# ${url.title}\n\n`
    pageContent += `> ${url.description}\n\n`
    pageContent += `## Overview\n\n`
    pageContent += `This content is from ${url.url}\n\n`
    pageContent += `## Content\n\n`
    
    // Add the actual content with proper formatting
    if (url.content) {
      const cleanContent = url.content
        .replace(/\n{3,}/g, '\n\n')
        .trim()
      pageContent += cleanContent
    } else {
      pageContent += 'No content available for this page.'
    }
    
    pageContent += `\n\n---\n\n`
    pageContent += `*Generated by SideGSO on ${new Date().toISOString()}*\n`
    pageContent += `*Source: ${url.url}*`
    
    // Add individual file
    filesToInsert.push({
      job_id: jobId,
      file_type: 'llms.txt',
      file_path: `${job.domain}/${pageName}-llms.txt`,
      file_size: new TextEncoder().encode(pageContent).length,
      content: pageContent
    })
  }
  
  // Generate index file
  let indexContent = `# ${job.domain} - SideGSO Files\n\n`
  indexContent += `Generated: ${new Date().toISOString()}\n`
  indexContent += `Total Pages: ${urls.length}\n\n`
  indexContent += `## Available LLMs.txt Files\n\n`
  
  for (const url of urls) {
    const urlPath = new URL(url.url).pathname
    const pageName = urlPath.split('/').filter(Boolean).pop() || 'index'
    indexContent += `- **${pageName}-llms.txt**: [${url.title}](${url.url})\n`
    indexContent += `  - ${url.description}\n\n`
  }
  
  indexContent += `\n## Summary\n\n`
  indexContent += `This collection contains ${urls.length} pages from ${job.domain}.\n`
  indexContent += `Total content size: ${urls.reduce((sum, u) => sum + (u.content?.length || 0), 0)} characters.\n`
  
  // Add index file
  filesToInsert.push({
    job_id: jobId,
    file_type: 'llms.txt',
    file_path: `${job.domain}/index-llms.txt`,
    file_size: new TextEncoder().encode(indexContent).length,
    content: indexContent
  })
  
  // Generate combined llms.txt
  let combinedContent = `# ${job.domain} - Combined LLMs.txt\n\n`
  combinedContent += `Generated: ${new Date().toISOString()}\n`
  combinedContent += `Total Pages: ${urls.length}\n\n`
  combinedContent += `---\n\n`
  
  for (const url of urls) {
    combinedContent += `## ${url.title}\n\n`
    combinedContent += `URL: ${url.url}\n`
    combinedContent += `Description: ${url.description}\n\n`
    if (url.content) {
      combinedContent += url.content.substring(0, 10000) // Limit each page to 10K chars
      if (url.content.length > 10000) {
        combinedContent += '\n\n[Content truncated...]'
      }
    }
    combinedContent += `\n\n---\n\n`
  }
  
  // Add combined file
  filesToInsert.push({
    job_id: jobId,
    file_type: 'llms.txt',
    file_path: `${job.domain}/combined-llms.txt`,
    file_size: new TextEncoder().encode(combinedContent).length,
    content: combinedContent
  })
  
  // Generate llms-full.txt (complete content)
  let fullContent = `# ${job.domain} - Full Content\n\n`
  fullContent += `Generated: ${new Date().toISOString()}\n`
  fullContent += `Total Pages: ${urls.length}\n\n`
  fullContent += `---\n\n`
  
  for (const url of urls) {
    fullContent += `## ${url.title}\n\n`
    fullContent += `URL: ${url.url}\n`
    fullContent += `Description: ${url.description}\n\n`
    fullContent += `### Content\n\n`
    if (url.content) {
      fullContent += url.content
    } else {
      fullContent += 'No content available.'
    }
    fullContent += `\n\n---\n\n`
  }
  
  // Add full content file
  filesToInsert.push({
    job_id: jobId,
    file_type: 'llms-full.txt',
    file_path: `${job.domain}/llms-full.txt`,
    file_size: new TextEncoder().encode(fullContent).length,
    content: fullContent
  })
  
  // Insert all files
  const { error: insertError } = await supabase
    .from('generated_files')
    .insert(filesToInsert)
  
  if (insertError) {
    console.error('Error inserting files:', insertError)
  } else {
    console.log(`Generated ${filesToInsert.length} files for job ${jobId}`)
  }
  
  // Update job status
  await supabase
    .from('crawl_jobs')
    .update({
      status: 'completed',
      completed_at: new Date().toISOString(),
      urls_processed: urls.length
    })
    .eq('id', jobId)
}